# LocalMediaSuite 2.0 ğŸš€

**LocalMediaSuite 2.0** ist die ultimative universelle KI-Suite fÃ¼r lokale Medien-Generation. Das System erkennt **automatisch alle Modellformate** und kann Ã¼ber 50+ verschiedene Modelltypen verwenden - von Stable Diffusion bis zu modernen LLMs.

**ğŸ¯ Neu in Version 2.0:**
- **Universelle Modell-Erkennung**: Automatische Erkennung aller gÃ¤ngigen Formate
- **Multi-Backend-Support**: Diffusers, ONNX, llama.cpp, Transformers, Ollama
- **Erweiterte Katalog-Datenbank**: 50+ vorkonfigurierte Modelle zum Download
- **Intelligente KompatibilitÃ¤tsprÃ¼fung**: Hardware-Optimierung und Empfehlungen
- **Unified Generation API**: Ein Endpunkt fÃ¼r alle Generierungsarten

---

## âœ¨ UnterstÃ¼tzte Modellformate

### ğŸ–¼ï¸ **Bildmodelle**
- **Diffusers Repositories** (.diffusers-Ordner mit model_index.json)
- **SafeTensors** (.safetensors) - Empfohlenes sicheres Format
- **Checkpoints** (.ckpt, .pt, .pth) - Legacy PyTorch Format
- **ONNX** (.onnx) - Optimiert fÃ¼r CPU/GPU-Inferenz
- **TensorRT** (.trt) - NVIDIA GPU-optimiert
- **CoreML** (.mlmodel) - Apple Silicon optimiert

**UnterstÃ¼tzte Architekturen:**
- Stable Diffusion 1.5 (512x512)
- Stable Diffusion 2.0/2.1 (768x768)  
- **Stable Diffusion XL** (1024x1024+)
- Stable Diffusion 3 (Neueste Generation)
- ControlNet (PrÃ¤zise Kontrolle)
- Inpainting-Modelle

### ğŸ¥ **Videomodelle**
- **Stable Video Diffusion** (img2vid, img2vid-xt)
- **AnimateDiff** (SD-kompatible Animation)
- **I2VGen-XL** (Text-zu-Video)
- Alle Diffusers-kompatiblen Video-Pipelines

### ğŸ¤– **LLM-Modelle**
- **GGUF** (.gguf) - llama.cpp Format (quantisiert)
- **Transformers** (Hugging Face Format)
- **Ollama** (Externe Integration)
- **SafeTensors LLMs** (Moderne LLM-Checkpoints)

**UnterstÃ¼tzte LLM-Architekturen:**
- Llama 3/3.1 (8B, 70B)
- Mixtral 8x7B (Mixture-of-Experts)
- Qwen2 (Mehrsprachig)
- Phi-3 (Kompakt aber leistungsstark)
- Gemma, Yi, Mistral und viele mehr

---

## ğŸš€ Erweiterte Installation

### Systemanforderungen

**Minimum:**
- Python 3.9+
- 8GB RAM
- 10GB freier Speicher

**Empfohlen:**
- Python 3.10+
- NVIDIA GPU mit 8GB+ VRAM (RTX 3070/4060 oder besser)
- 16GB+ RAM
- 50GB+ freier Speicher (fÃ¼r mehrere Modelle)

### Setup-Optionen

**Option 1: Standard-Installation**
```bash
git clone <repository-url>
cd LocalMediaSuite
python -m venv .venv
source .venv/bin/activate  # Linux/Mac
# oder: .\.venv\Scripts\activate  # Windows

pip install -r requirements.txt
python start.py
```

**Option 2: GPU-Optimierte Installation**
```bash
# Nach Standard-Installation:

# NVIDIA CUDA 11.8+ (empfohlen)
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# Oder CUDA 12.1+
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121

# AMD ROCm (experimentell)
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/rocm5.6

# Apple Silicon (nativer Support)
# Torch automatisch MPS-optimiert
```

**Option 3: Docker-Installation**
```bash
# TODO: Docker-Support kommt in v2.1
docker run -p 3000:3000 -v ./models:/app/models localmediasuite:latest
```

---

## ğŸ“ Erweiterte Verzeichnisstruktur

```
LocalMediaSuite/
â”œâ”€â”€ ğŸ“‚ models/                  # KI-Modelle (automatisch erkannt)
â”‚   â”œâ”€â”€ ğŸ“‚ image/              # Bild-Modelle
â”‚   â”‚   â”œâ”€â”€ ğŸ“‚ realistic-vision-v6/  # Diffusers-Repository
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ dreamshaper-8.safetensors  # Single-File
â”‚   â”‚   â””â”€â”€ ğŸ“„ sdxl-base.ckpt  # Legacy Checkpoint
â”‚   â”œâ”€â”€ ğŸ“‚ video/              # Video-Modelle  
â”‚   â”‚   â”œâ”€â”€ ğŸ“‚ stable-video-diffusion/
â”‚   â”‚   â””â”€â”€ ğŸ“‚ animatediff-motion-module/
â”‚   â””â”€â”€ ğŸ“‚ llm/                # LLM-Modelle
â”‚       â”œâ”€â”€ ğŸ“‚ llama-3-8b-instruct/  # Transformers
â”‚       â”œâ”€â”€ ğŸ“„ mixtral-8x7b-q4.gguf  # Quantisiert
â”‚       â””â”€â”€ ğŸ“„ qwen2-7b.safetensors
â”œâ”€â”€ ğŸ“‚ server/                 # FastAPI Backend
â”‚   â”œâ”€â”€ ğŸ“„ models_registry.py  # Universelle Erkennung
â”‚   â”œâ”€â”€ ğŸ“„ model_catalog.py    # 50+ Modell-Datenbank
â”‚   â””â”€â”€ ğŸ“„ universal_generate.py # Multi-Backend Generator
â”œâ”€â”€ ğŸ“‚ web/                    # Frontend
â””â”€â”€ ğŸ“‚ outputs/                # Generierte Inhalte
```

---

## ğŸ¯ Universelle API

### `/api/generate/universal` - Ein Endpunkt fÃ¼r alles

```javascript
// Bild generieren (SD 1.5)
fetch('/api/generate/universal', {
  method: 'POST',
  body: new FormData({
    prompt: 'Beautiful landscape at sunset, photorealistic, 8k',
    model_id: 'image:realistic-vision-v6',
    mode: 'text2img',
    width: 1024,
    height: 768
  })
})

// Video generieren (SVD)
fetch('/api/generate/universal', {
  method: 'POST', 
  body: new FormData({
    model_id: 'video:stable-video-diffusion',
    mode: 'img2video',
    init_image: imageFile,
    num_frames: 25,
    fps: 24
  })
})

// Text generieren (Llama 3)
fetch('/api/generate/universal', {
  method: 'POST',
  body: new FormData({
    prompt: 'Explain quantum computing in simple terms',
    model_id: 'llm:llama-3-8b-instruct', 
    mode: 'text',
    max_tokens: 512,
    temperature: 0.8
  })
})
```

### Erweiterte Catalog-API

```javascript
// Alle verfÃ¼gbaren Modelle durchsuchen
GET /api/catalog

// Hardware-optimierte Empfehlungen
GET /api/recommendations?max_vram=8&beginner_friendly=true

// Modell-Suche
GET /api/search?q=realistic%20portrait

// KompatibilitÃ¤tsprÃ¼fung  
GET /api/compatibility/stabilityai/stable-diffusion-xl-base-1.0

// Ein-Klick-Installation
POST /api/models/install
{
  "repo_id": "SG161222/Realistic_Vision_V6.0_B1_noVAE",
  "type": "image", 
  "name": "Realistic Vision V6"
}
```

---

## ğŸ› ï¸ Erweiterte Konfiguration

### Hardware-Optimierung

**GPU-Speicher-Optimierung (config/settings.json):**
```json
{
  "generation": {
    "enable_model_cpu_offload": true,
    "enable_sequential_cpu_offload": true, 
    "enable_attention_slicing": true,
    "enable_memory_efficient_attention": true,
    "use_safetensors": true
  },
  "hardware": {
    "max_batch_size": 1,
    "precision": "fp16",
    "compile_models": false
  }
}
```

**Multi-GPU-Setup:**
```json
{
  "hardware": {
    "device_map": "auto",
    "gpu_ids": [0, 1],
    "load_balancing": "memory_usage"
  }
}
```

### Modell-Sidecar-Dateien

Erstelle `.model.json` neben jedem Modell fÃ¼r erweiterte Metadaten:

```json
{
  "name": "Realistic Vision V6.0",
  "architecture": "sd15", 
  "precision": "fp16",
  "nsfw_capable": true,
  "recommended_use": "Fotorealistische Portraits und Szenen",
  "tags": ["photorealistic", "portrait", "versatile"],
  "modalities": ["text2img", "img2img"],
  "requirements": ["cuda_4gb"],
  "optimal_settings": {
    "steps": 28,
    "guidance_scale": 7.0,
    "resolution": "1024x768"
  },
  "negative_prompts": [
    "blurry, low quality, distorted, bad anatomy"
  ]
}
```

---

## ğŸ“Š Performance-Benchmarks

### Typische Generierungszeiten (RTX 4070)

| Modell-Typ | AuflÃ¶sung | Zeit | VRAM |
|------------|-----------|------|------|
| SD 1.5 | 512x512 | 3-5s | 3.2GB |
| SDXL | 1024x1024 | 8-12s | 6.8GB |
| SVD (25 frames) | 1024x576 | 45-60s | 11.2GB |
| Llama 3 8B (GGUF Q4) | - | 50 tok/s | 5.1GB |

### Speicherbedarf-Ãœbersicht

| Kategorie | Klein | Mittel | GroÃŸ |
|-----------|-------|---------|------|
| **SD 1.5** | 2.1GB | 4.2GB | 7.1GB |
| **SDXL** | 6.5GB | 13.5GB | 20.1GB |
| **LLM (FP16)** | 7GB (3B) | 15GB (7B) | 30GB (13B) |
| **LLM (GGUF Q4)** | 2GB (3B) | 4GB (7B) | 8GB (13B) |

---

## ğŸ”§ Erweiterte Features

### Batch-Generierung
```python
# Mehrere Bilder gleichzeitig
POST /api/models/batch-install
[
  {"repo_id": "SG161222/Realistic_Vision_V6.0", "type": "image"},
  {"repo_id": "stabilityai/stable-video-diffusion-img2vid-xt", "type": "video"}
]
```

### Intelligente Hardware-Erkennung
```python
GET /api/models/compatibility-check
# Automatische VRAM/RAM-Analyse
# Modell-Empfehlungen basierend auf Hardware
# Performance-OptimierungsvorschlÃ¤ge
```

### Erweiterte Prompt-Engineering
- **Kategoriebasierte VorschlÃ¤ge** fÃ¼r jeden Modelltyp
- **Negative Prompt-Datenbank**
- **Stil-Transfer-Presets**
- **Automatische Prompt-Optimierung**

### Multi-Modal Workflows
```javascript
// Text â†’ Bild â†’ Video Pipeline
const image = await generateImage(textPrompt)
const video = await generateVideo(image)
const description = await generateText(video)
```

---

## ğŸ” Fehlerbehebung 2.0

### HÃ¤ufige Probleme & LÃ¶sungen

**Modell wird nicht erkannt:**
```bash
# PrÃ¼fe Modell-Format
GET /api/models  # Zeigt alle erkannten Modelle

# Erstelle Sidecar-Datei
echo '{"type":"image","architecture":"sd15"}' > model.model.json
```

**"Pipeline not supported":**
```python
# PrÃ¼fe verfÃ¼gbare Backends
GET /api/info  # Zeigt Backend-Status

# Installiere fehlende AbhÃ¤ngigkeiten
pip install diffusers transformers onnxruntime
```

**CUDA out of memory:**
```json
// Aktiviere Memory-Optimierungen
{
  "generation": {
    "enable_model_cpu_offload": true,
    "enable_sequential_cpu_offload": true,
    "max_batch_size": 1
  }
}
```

**Slow generation on CPU:**
```bash
# Verwende ONNX-optimierte Modelle
GET /api/recommendations?format=onnx

# Oder quantisierte Versionen
GET /api/search?q=gguf
```

---

## ğŸš€ Roadmap v2.1+

### Geplante Features

- **ğŸ³ Docker-Support** mit GPU-Passthrough
- **â˜ï¸ Cloud-Integration** (optional fÃ¼r groÃŸe Modelle)
- **ğŸ¨ ControlNet-Suite** (Pose, Depth, Canny, etc.)
- **ğŸ“± Mobile App** (Remote-Steuerung)
- **ğŸ”„ Model Conversion** (ONNX, TensorRT Export)
- **ğŸ“Š Advanced Analytics** (Performance-Tracking)
- **ğŸ¤ Multi-User-Support** (Team-Features)
- **ğŸ­ Face/Style Consistency** (Character-Erhaltung)

### Community & BeitrÃ¤ge

**Modell-Datenbank erweitern:**
```python
# FÃ¼ge neue Modelle zur Katalog-Datenbank hinzu
# server/model_catalog.py - CATALOG_MODELS Liste
```

**Backend-Support hinzufÃ¼gen:**
```python
# Implementiere neuen Backend in
# server/routes/universal_generate.py
```

---

## ğŸ“ Support & Community

- **ğŸ“§ Issues:** [GitHub Issues](https://github.com/your-repo/issues)
- **ğŸ’¬ Diskussionen:** [GitHub Discussions](https://github.com/your-repo/discussions)  
- **ğŸ“– Wiki:** [VollstÃ¤ndige Dokumentation](https://github.com/your-repo/wiki)
- **ğŸ¥ Tutorials:** [YouTube-Kanal](https://youtube.com/@localmediasuite)

---

**â­ LocalMediaSuite 2.0 - Die Zukunft der lokalen KI-Generation ist da!**

*Keine Cloud, keine Limits, keine Filter - Nur pure KI-Power auf deiner Hardware.*# LocalMediaSuite

**LocalMediaSuite** ist eine vollstÃ¤ndige Windows/Linux/macOS-basierte LÃ¶sung, um im Heimnetzwerk einen lokalen KI-Server mit moderner WeboberflÃ¤che bereitzustellen. Der Server lÃ¤uft auf deinem PC, hostet die Web-UI fÃ¼r alle GerÃ¤te im Netzwerk und ermÃ¶glicht die Generierung, Verwaltung und Wiedergabe von Bildern und Videos.

**Alle Berechnungen laufen ausschlieÃŸlich lokal** â€“ keine Cloud, keine externen APIs, keine Filter oder Sperren.

## âœ¨ Features

### ğŸ¨ Medien-Generierung
- **Bild-Generation**: Textâ†’Bild, Bildâ†’Bild (Stable Diffusion)
- **Video-Generation**: Textâ†’Video, Bildâ†’Video (Stable Video Diffusion)
- **SFW & NSFW**: Separate Modi ohne Filter oder EinschrÃ¤nkungen
- **Erweiterte Parameter**: AuflÃ¶sung, Steps, Guidance, Strength, etc.

### ğŸ¯ Benutzerfreundlichkeit
- **Intelligente Prompts**: Kategoriebasierte VorschlÃ¤ge und Zufalls-Generator
- **Remix-Funktion**: Verwende vorherige Ergebnisse als Basis
- **Live-Galerie**: Automatische Anzeige neuer Generierungen
- **Modell-Management**: Automatische Erkennung und Ein-Klick-Installation

### ğŸ–¥ï¸ Moderne OberflÃ¤che
- **Responsive Design**: Funktioniert auf Desktop, Tablet und Smartphone
- **Dark Theme**: Augenfreundliche OberflÃ¤che
- **Live-Status**: Desktop-UI zeigt Systemauslastung und laufende VorgÃ¤nke
- **Integrierter Player**: Direktwiedergabe von Bildern und Videos

### âš™ï¸ Technische Highlights
- **FastAPI Backend**: Moderne, asynchrone API
- **Diffusers Integration**: UnterstÃ¼tzt alle gÃ¤ngigen Stable Diffusion Modelle
- **Hugging Face Hub**: Automatische Modell-Installation
- **Ollama Support**: LLM-Integration fÃ¼r zukÃ¼nftige Features
- **Broadcasting**: Live-Updates zwischen Browser-Tabs

## ğŸš€ Schnellstart

### 1. Voraussetzungen

**Python 3.9 oder hÃ¶her** ist erforderlich.

**FÃ¼r GPU-Beschleunigung (empfohlen):**
- NVIDIA GPU mit CUDA 11.8+ (RTX-Serie empfohlen)
- Oder AMD GPU mit ROCm 5.6+
- Oder Apple Silicon (M1/M2/M3)

### 2. Installation

```bash
# Repository klonen
git clone <repository-url>
cd LocalMediaSuite

# Python Virtual Environment erstellen (empfohlen)
python -m venv .venv

# Virtual Environment aktivieren
# Windows PowerShell:
.\.venv\Scripts\activate
# Linux/macOS:
source .venv/bin/activate

# AbhÃ¤ngigkeiten installieren
pip install -r requirements.txt

# FÃ¼r optimale GPU-Performance (NVIDIA):
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# Erste AusfÃ¼hrung
python start.py
```

### 3. Zugriff

Nach dem Start Ã¶ffne einen Browser und navigiere zu:
- **Lokal**: `http://localhost:3000`
- **Im Netzwerk**: `http://<deine-ip>:3000` (wird beim Start angezeigt)

## ğŸ“ Projektstruktur

```
LocalMediaSuite/
â”œâ”€â”€ ğŸ“‚ config/              # Konfigurationsdateien
â”‚   â”œâ”€â”€ suggestions.json    # Prompt-VorschlÃ¤ge
â”‚   â””â”€â”€ settings.json       # Benutzereinstellungen
â”œâ”€â”€ ğŸ“‚ models/              # KI-Modelle (leer bei Start)
â”‚   â”œâ”€â”€ image/              # Bild-Modelle (.safetensors, .ckpt)
â”‚   â”œâ”€â”€ video/              # Video-Modelle (SVD)
â”‚   â””â”€â”€ llm/                # LLM-Modelle (Ollama)
â”œâ”€â”€ ğŸ“‚ outputs/             # Generierte Dateien
â”‚   â”œâ”€â”€ images/             # Generierte Bilder
â”‚   â””â”€â”€ videos/             # Generierte Videos
â”œâ”€â”€ ğŸ“‚ server/              # FastAPI Backend
â”‚   â”œâ”€â”€ routes/             # API-Endpunkte
â”‚   â”œâ”€â”€ models_registry.py  # Modell-Verwaltung
â”‚   â”œâ”€â”€ settings.py         # Einstellungs-System
â”‚   â””â”€â”€ server.py           # Haupt-Server
â”œâ”€â”€ ğŸ“‚ web/                 # Frontend (HTML/CSS/JS)
â”‚   â”œâ”€â”€ assets/             # Styles, Scripts, Icons
â”‚   â”œâ”€â”€ image.html          # Bild-Generator (SFW)
â”‚   â”œâ”€â”€ image_nsfw.html     # Bild-Generator (NSFW)
â”‚   â”œâ”€â”€ video.html          # Video-Generator (SFW)
â”‚   â”œâ”€â”€ video_nsfw.html     # Video-Generator (NSFW)
â”‚   â”œâ”€â”€ gallery.html        # Galerie mit Player
â”‚   â”œâ”€â”€ catalog.html        # Modell-Katalog
â”‚   â””â”€â”€ settings.html       # Einstellungen
â”œâ”€â”€ requirements.txt        # Python-AbhÃ¤ngigkeiten
â”œâ”€â”€ start.py               # Startup-Script mit Desktop-UI
â””â”€â”€ README.md              # Diese Datei
```

## ğŸ¯ Erste Schritte

### 1. Modelle installieren

**Option A: Ãœber den Katalog (empfohlen)**
1. Ã–ffne `http://localhost:3000/catalog.html`
2. Klicke auf "Installieren" bei den empfohlenen Modellen
3. Warte bis Download abgeschlossen ist

**Option B: Manuell**
```bash
# Beispiel: Realistic Vision v6.0 fÃ¼r Bilder
mkdir -p models/image
# Lade Modell von Hugging Face oder CivitAI herunter
# und kopiere es nach models/image/

# Beispiel: Stable Video Diffusion fÃ¼r Videos
mkdir -p models/video
# Verwende git-lfs fÃ¼r groÃŸe Diffusers-Modelle:
cd models/video
git clone https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt
```

### 2. Erstes Bild generieren

1. Gehe zu `http://localhost:3000/image.html`
2. WÃ¤hle ein installiertes Modell aus
3. Gib einen Prompt ein: "A beautiful landscape at sunset, photorealistic, 8k"
4. Klicke auf "Generieren"
5. Das Ergebnis erscheint automatisch in der Galerie

### 3. Erstes Video generieren

1. Gehe zu `http://localhost:3000/video.html`
2. WÃ¤hle ein Video-Modell (SVD)
3. WÃ¤hle "Text â†’ Video" oder "Bild â†’ Video"
4. Gib einen Prompt ein oder wÃ¤hle ein Startbild
5. Klicke auf "Generieren"

## âš™ï¸ Konfiguration

### GPU-Einstellungen

Die Software erkennt automatisch verfÃ¼gbare GPUs:
- **NVIDIA CUDA**: Automatisch genutzt wenn verfÃ¼gbar
- **AMD ROCm**: Manuell installieren mit `pip install torch --index-url https://download.pytorch.org/whl/rocm5.6`
- **Apple Silicon**: Native UnterstÃ¼tzung Ã¼ber MPS

### Speicher-Optimierung

FÃ¼r Systeme mit weniger als 16GB RAM:
```python
# In server/routes/generate.py anpassen:
pipe.enable_model_cpu_offload()
pipe.enable_attention_slicing()
pipe.enable_sequential_cpu_offload()  # FÃ¼r sehr wenig VRAM
```

### Erweiterte Einstellungen

Bearbeite `config/settings.json`:
```json
{
  "paths": {
    "images": "outputs/images",
    "videos": "outputs/videos",
    "image": "models/image",
    "video": "models/video"
  },
  "defaults": {
    "img_resolution": "1024x768",
    "vid_resolution": "1024x576",
    "vid_fps": 24,
    "vid_length_sec": 3
  },
  "ui": {
    "theme": "dark"
  }
}
```

## ğŸ”§ API-Dokumentation

### Wichtige Endpunkte

- `GET /api/models` - Liste aller verfÃ¼gbaren Modelle
- `POST /api/generate/image` - Bild generieren
- `POST /api/generate/video` - Video generieren
- `GET /api/files?type=image|video` - Galerie-Inhalte
- `GET /api/status` - Systemstatus
- `GET /api/ops` - Laufende Operationen

### VollstÃ¤ndige API-Docs

Nach dem Start verfÃ¼gbar unter: `http://localhost:3000/docs`

## ğŸ¨ Modell-Empfehlungen

### Bild-Modelle (Diffusers/SafeTensors)

**Fotorealistisch:**
- `SG161222/Realistic_Vision_V6.0_B1_noVAE` - Ultra-realistisch
- `stabilityai/stable-diffusion-xl-base-1.0` - SDXL Base
- `runwayml/stable-diffusion-v1-5` - SD 1.5 Standard

**KÃ¼nstlerisch:**
- `Lykon/DreamShaper` - Vielseitig fÃ¼r alle Stile
- `andite/anything-v4.0` - Anime/Manga
- `nitrosocke/Arcane-Diffusion` - Comic-Stil

### Video-Modelle

**Standard:**
- `stabilityai/stable-video-diffusion-img2vid-xt` - Bester Allrounder
- `stabilityai/stable-video-diffusion-img2vid` - Kompaktere Version

### Installation Ã¼ber Hugging Face Hub

```bash
# In Python-Console oder Script:
from huggingface_hub import snapshot_download

# Modell herunterladen
snapshot_download(
    repo_id="SG161222/Realistic_Vision_V6.0_B1_noVAE",
    local_dir="models/image/Realistic_Vision_V6",
    repo_type="model"
)
```

## ğŸ” Fehlerbehebung

### HÃ¤ufige Probleme

**"CUDA out of memory":**
```python
# Reduziere Batch-Size oder nutze CPU-Offloading
pipe.enable_model_cpu_offload()
pipe.enable_sequential_cpu_offload()
```

**"No module named 'torch'":**
```bash
pip install torch torchvision torchaudio
```

**"ffmpeg not found" (fÃ¼r Videos):**
```bash
# Windows: Scoop installieren, dann:
scoop install ffmpeg

# macOS:
brew install ffmpeg

# Ubuntu/Debian:
sudo apt install ffmpeg
```

**Server startet nicht:**
```bash
# Port bereits belegt - anderen Port verwenden:
python start.py --port 8080

# AbhÃ¤ngigkeiten prÃ¼fen:
pip install -r requirements.txt
```

### Debug-Modus

```bash
# Mit detaillierten Logs:
python start.py --dev

# Ohne Desktop-UI (fÃ¼r Server):
python start.py --headless
```

## ğŸ¤ Beitragen

Dieses Projekt ist Open Source! BeitrÃ¤ge sind willkommen:

1. Fork das Repository
2. Erstelle einen Feature-Branch: `git checkout -b feature/neue-funktion`
3. Commit deine Ã„nderungen: `git commit -m 'Add neue Funktion'`
4. Push zum Branch: `git push origin feature/neue-funktion`
5. Erstelle einen Pull Request

## ğŸ“„ Lizenz

MIT License - siehe [LICENSE](LICENSE) Datei fÃ¼r Details.

## ğŸ™ Danksagungen

- **Stability AI** - FÃ¼r Stable Diffusion und SVD
- **Hugging Face** - FÃ¼r Diffusers und Model Hub
- **FastAPI Team** - FÃ¼r das exzellente Web-Framework
- **Community** - FÃ¼r Feedback und BeitrÃ¤ge

---

**â­ GefÃ¤llt dir LocalMediaSuite? Gib dem Projekt einen Stern auf GitHub!**

**ğŸ› Probleme gefunden?** [Ã–ffne ein Issue](https://github.com/dein-repo/LocalMediaSuite/issues)

**ğŸ’¬ Fragen?** Schau in die [Discussions](https://github.com/dein-repo/LocalMediaSuite/discussions)